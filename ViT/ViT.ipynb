{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"vit_model\", name=\"MultiHeadAttention\")\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, index=0):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.index = index\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim, name = f'query_{index}')\n",
    "        self.key_dense = layers.Dense(embed_dim, name = f\"key_{index}\")\n",
    "        self.value_dense = layers.Dense(embed_dim, name = f'value_{index}')\n",
    "        self.combine_heads = layers.Dense(embed_dim, name = f'out_{index}')\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(\n",
    "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
    "        )\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"embed_dim\": self.embed_dim,\n",
    "                       \"num_heads\": self.num_heads  ,\n",
    "                       \"index\": self.index                     \n",
    "                       })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"vit_model\", name=\"TransformerBlock\")\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1, index = 0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.index = index\n",
    "        self.name = f\"transformer_block_{self.index}\"\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, self.index)\n",
    "        self.mlp = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(mlp_dim, activation=keras.activations.gelu, name =f'enc_{self.index}_d1'),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(embed_dim, name = f'enc_{index}_d2'),\n",
    "                layers.Dropout(dropout),\n",
    "            ],\n",
    "            name='mlp'\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"mlp_dim\": self.mlp_dim,\n",
    "                       \"embed_dim\": self.embed_dim,\n",
    "                       \"num_heads\": self.num_heads,\n",
    "                        \"dropout\": self.dropout,\n",
    "                        \"att\": self.att,\n",
    "                        \"index\": self.index\n",
    "                       })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        config['att'] = keras.layers.deserialize(config[\"att\"])\n",
    "        return cls(**config)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inputs_norm = self.layernorm1(inputs)\n",
    "        attn_output = self.att(inputs_norm)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = attn_output + inputs\n",
    "\n",
    "        out1_norm = self.layernorm2(out1)\n",
    "        mlp_output = self.mlp(out1_norm)\n",
    "        mlp_output = self.dropout2(mlp_output, training=training)\n",
    "        return mlp_output + out1\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"vit_model\", name=\"ViT\")\n",
    "class ViT(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        depth,\n",
    "        num_classes,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "        mlp_dim,\n",
    "        dropout=0.1,\n",
    "        name = \"ViT\",\n",
    "        include_head = True,\n",
    "        channels = 3\n",
    "    ):\n",
    "        super(ViT, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout\n",
    "        self.name = name\n",
    "        self.input_shape = (image_size, image_size, channels)\n",
    "        self.include_head = include_head\n",
    "        self.mlp_head = None\n",
    "        self.channels = channels\n",
    "\n",
    "        # Create patch embedding/flattening layers\n",
    "        self.conv_proj = layers.Conv2D(filters = self.hidden_dim, \n",
    "                                       kernel_size = self.patch_size,\n",
    "                                       strides=self.patch_size,\n",
    "                                       name = \"patches_encoder\")\n",
    "        self.patch_flatten = layers.Reshape((self.num_patches, \n",
    "                                             self.hidden_dim),\n",
    "                                             name = \"patches_flatten\")\n",
    "        \n",
    "        self.add_cls_emb = layers.Concatenate(name = 'concat_pos_embed', axis = 1)\n",
    "\n",
    "        self.class_emb = tf.Variable(\n",
    "            initial_value=tf.random.normal([1, 1, self.hidden_dim]),\n",
    "            trainable=True,\n",
    "            name=\"class_emb\",\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        self.pos_emb = tf.Variable(\n",
    "            initial_value=tf.random.normal([1, self.num_patches + 1, self.hidden_dim]),\n",
    "            trainable=True,\n",
    "            name=\"pos_emb\",\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = [\n",
    "            TransformerBlock(self.hidden_dim, self.num_heads, self.mlp_dim, self.dropout_rate, index)\n",
    "            for index in range(self.depth)\n",
    "        ]\n",
    "        \n",
    "        if self.include_head:\n",
    "            # Classification head\n",
    "            self.mlp_head = tf.keras.Sequential(\n",
    "                [\n",
    "                    layers.LayerNormalization(epsilon=1e-6, name = 'mlp_head_ln'),\n",
    "                    layers.Dense(self.mlp_dim, activation=tf.keras.activations.tanh, name = 'mlp_head_d1'),\n",
    "                    layers.Dense(self.num_classes, name= 'classifier'),\n",
    "                ], \n",
    "                name = 'mlp_head'\n",
    "            )\n",
    "\n",
    "    def build_functional(self):\n",
    "        input_layer = layers.Input(shape=(self.image_size, self.image_size, self.channels), name = 'Input')\n",
    "        output_layer = self(input_layer)\n",
    "        return keras.Model(inputs=input_layer, outputs=output_layer, name = self.name)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"image_size\": self.image_size,\n",
    "                \"patch_size\": self.patch_size,\n",
    "                \"hidden_dim\": self.hidden_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"depth\": self.depth,\n",
    "                \"mlp_dim\": self.mlp_dim,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"dropout\": self.dropout_rate,\n",
    "                \"name\": self.name,\n",
    "                \"include_head\": self.include_head,\n",
    "                \"channels\": self.channels\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        return cls(**config)\n",
    "        \n",
    "    def call(self, inputs, training= False):\n",
    "        # Create encoded patches\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        x = self.conv_proj(inputs)\n",
    "        x = self.patch_flatten(x)\n",
    "\n",
    "        # Adjust and prepend class embedding\n",
    "        cls_emb = tf.broadcast_to(\n",
    "            self.class_emb, [batch_size, 1, self.hidden_dim]\n",
    "        )\n",
    "        x = self.add_cls_emb([cls_emb, x])\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.pos_emb\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        for block in self.transformer_encoder:\n",
    "            x = block(x, training= training)\n",
    "            \n",
    "        # Classify\n",
    "        if self.include_head:\n",
    "            x = self.mlp_head(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create ViT-B16 and verify we get the same number of parameters as the model from the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"image_size\": 224,\n",
    "    \"patch_size\": 16,\n",
    "    \"depth\": 12,\n",
    "    \"num_heads\": 12,\n",
    "    \"hidden_dim\": 768,\n",
    "    \"mlp_dim\": 3072,\n",
    "    \"num_classes\": 1000,\n",
    "    \"dropout\": 0.1,\n",
    "    \"name\": \"ViT-B16\",\n",
    "    \"include_head\": True\n",
    "}\n",
    "\n",
    "vit_model = ViT(config['image_size'],\n",
    "              config['patch_size'],\n",
    "              config['depth'],\n",
    "              config['num_classes'],\n",
    "              config['hidden_dim'],\n",
    "              config['num_heads'],\n",
    "              config['mlp_dim'],\n",
    "              config['dropout'],\n",
    "              config['name'],\n",
    "              include_head= config['include_head'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build as functional model and get output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build as functional and print the model summary\n",
    "vit_model = vit_model.build_functional()\n",
    "vit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a small ViT model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Load dataset and prepare model\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "ds, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (28, 28)) \n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, tf.cast(label, tf.float32)\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(preprocess_image)\n",
    "ds_test = ds_test.map(preprocess_image)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "ds_train = ds_train.batch(batch_size).shuffle(10000).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = ViT(\n",
    "    image_size=28,\n",
    "    patch_size=4,\n",
    "    depth=6,\n",
    "    num_classes=10,\n",
    "    hidden_dim=64,\n",
    "    num_heads=4,\n",
    "    mlp_dim=128,\n",
    "    dropout=0.1,\n",
    "    name=\"ViT_MNIST\",\n",
    "    include_head=True,\n",
    "    channels=1\n",
    ").build_functional()\n",
    "\n",
    "print(model.summary())\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# Prepare training arguments\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "initial_learning_rate = 1e-3\n",
    "decay_steps = len(ds_train) * (150 - 10)\n",
    "alpha = 1e-5 / initial_learning_rate\n",
    "warmup_steps = len(ds_train) * 10\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps, warmup_target=1e-5,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "optimizer = AdamW(learning_rate=lr_schedule, weight_decay=5e-5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Train the model\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "history = model.fit(ds_train.take(1), validation_data=ds_test.take(1), epochs=1, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check metrics from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model on some random images from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predictions(model, dataset, ds_info, num_images=8):\n",
    "    class_names = {idx: name for idx, name in enumerate(ds_info.features['label'].names)}\n",
    "    \n",
    "    # Get a random sample of num_images from the dataset\n",
    "    random_indices = np.random.choice(len(dataset), size=num_images, replace=False)\n",
    "    ds_subset = dataset.unbatch().skip(random_indices[0]).take(num_images).batch(num_images)\n",
    "\n",
    "    # Make predictions\n",
    "    images, labels = next(iter(ds_subset))\n",
    "    predicted_logits = model.predict(images)\n",
    "    predicted_probabilities = tf.nn.softmax(predicted_logits, axis=-1)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=-1)\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow((images[i].numpy() * 255).astype(np.uint8))\n",
    "        if predicted_classes[i] == labels[i].numpy():\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        \n",
    "        plt.title(f'True: {class_names[labels[i].numpy()]}\\nPredicted: {class_names[predicted_classes[i]]}', color=color)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "random_predictions(model, ds_test, ds_info, num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = model.evaluate(ds_test)[1]\n",
    "print(f'Test accuracy: {round(test_accuracy, 2) * 100} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
