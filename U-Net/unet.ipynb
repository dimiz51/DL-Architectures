{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import keras\n",
    "import keras_cv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(num_classes):\n",
    "    if num_classes > 2:\n",
    "        loss_fn = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "        model_metric = keras.metrics.MeanIoU(\n",
    "            num_classes=num_classes,\n",
    "            sparse_y_true=False,\n",
    "            sparse_y_pred=False,\n",
    "            name='mean_iou'\n",
    "        )\n",
    "        model_metric2 = keras.metrics.CategoricalAccuracy()\n",
    "        activation = 'softmax'\n",
    "    else:\n",
    "        loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        model_metric = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], name='binary_iou')\n",
    "        model_metric2 = 'accuracy'\n",
    "        activation = 'sigmoid'\n",
    "    return loss_fn, model_metric, model_metric2, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create U-Net model with MobilenetV2 backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet with pre-trained MobilenetV2 encoder\n",
    "def unet_builder(img_size, num_classes, weights = None):\n",
    "    def decoder_block(inputs, skip, num_filters, block_name):\n",
    "        \"Decoder Block generator\"\n",
    "        x = layers.UpSampling2D(size=(2, 2), name=f\"{block_name}_upsampling\")(inputs)\n",
    "        x = layers.Conv2D(num_filters,\n",
    "                          (3, 3),\n",
    "                          padding='same',\n",
    "                          activation='relu',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          name=f\"{block_name}_conv\")(x)\n",
    "        x = layers.Concatenate(name=f\"{block_name}_concat\")([x, skip])\n",
    "        return x\n",
    "\n",
    "    # Input Layer\n",
    "    inputs = keras.Input(shape = img_size, name = 'Input')\n",
    "\n",
    "    # Encoder\n",
    "    encoder = tf.keras.applications.MobileNetV2(input_shape=img_size, include_top=False, input_tensor= inputs)\n",
    "    s1 = encoder.get_layer(\"block_1_expand_relu\").output\n",
    "    s2 = encoder.get_layer(\"block_3_expand_relu\").output\n",
    "    s3 = encoder.get_layer(\"block_6_expand_relu\").output\n",
    "    s4 = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "\n",
    "    # Bottleneck layer\n",
    "    b1 = encoder.get_layer(\"block_16_expand_relu\").output\n",
    "\n",
    "    # Freeze pre-trained backbone\n",
    "    if weights:\n",
    "        encoder.trainable = False\n",
    "\n",
    "    # Decoder\n",
    "    d1 = decoder_block(b1, s4, 512, \"decoder_d1\")\n",
    "    d2 = decoder_block(d1, s3, 256, \"decoder_d2\")\n",
    "    d3 = decoder_block(d2, s2, 128, \"decoder_d3\")\n",
    "    d4 = decoder_block(d3, s1, 64,  \"decoder_d4\")\n",
    "\n",
    "    loss_fn, model_metric, model_metric2, activation = get_configs(num_classes)\n",
    "\n",
    "    up = layers.UpSampling2D(size=(2, 2), name='upsampling')(d4)\n",
    "    outputs = layers.Conv2D(num_classes,\n",
    "                            (3, 3),\n",
    "                            padding='same',\n",
    "                            activation=activation,\n",
    "                            name='output')(up)\n",
    "\n",
    "    # Create the UNet model\n",
    "    model = keras.models.Model(inputs=inputs, outputs=[outputs])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss=loss_fn,\n",
    "        metrics=[model_metric,model_metric2],\n",
    "    )\n",
    "\n",
    "    if weights:\n",
    "       model.load_weights(weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation pipeline with Keras CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Augmentation pipeline------------\n",
    "color_augmentations = [\n",
    "    keras_cv.layers.RandomChannelShift(value_range=(0, 1), factor=0.2),\n",
    "    keras_cv.layers.RandomColorDegeneration(factor=0.5),\n",
    "    keras_cv.layers.RandomHue(factor=0.2, value_range=(0.5, 0.8)),\n",
    "]\n",
    "\n",
    "# Define spatial augmentations\n",
    "spatial_augmentations = [\n",
    "    keras_cv.layers.RandomFlip(mode='horizontal'),\n",
    "    keras_cv.layers.RandomFlip(mode='vertical'),\n",
    "    keras_cv.layers.RandomRotation(factor = 0.15),\n",
    "]\n",
    "\n",
    "image_only_augmentations = keras_cv.layers.RandomAugmentationPipeline(layers=color_augmentations ,\n",
    "                                                                      augmentations_per_image=2,\n",
    "                                                                      seed=np.random.randint(1e6))\n",
    "image_mask_augmentations = keras_cv.layers.RandomAugmentationPipeline(layers=spatial_augmentations,\n",
    "                                                                      augmentations_per_image=2,\n",
    "                                                                      seed=np.random.randint(1e6))\n",
    "\n",
    "def apply_augmentation_pipeline(inputs):\n",
    "    \"\"\"Apply the augmentation pipelines to images and masks\"\"\"\n",
    "    image = inputs[\"images\"]\n",
    "    mask = inputs[\"segmentation_masks\"]\n",
    "\n",
    "    # Apply color-based augmentations to images\n",
    "    augmented_image = image_only_augmentations(image)\n",
    "\n",
    "    # Apply spatial augmentations to both images and masks\n",
    "    augmented_image_mask = image_mask_augmentations(tf.concat([image, tf.cast(mask, dtype= tf.float32)], axis=-1))\n",
    "    augmented_image, augmented_mask = tf.split(augmented_image_mask, [3, 1], axis=-1)\n",
    "\n",
    "    return {\"images\": augmented_image, \"segmentation_masks\": tf.cast(augmented_mask, dtype= tf.uint8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the small fire detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset source: https://github.com/hayatkhan8660-maker/Fire_Seg_Dataset?tab=readme-ov-file\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "NUM_CLASSES = 2\n",
    "MODEL_WIDTH = 224\n",
    "MODEL_HEIGHT = 224\n",
    "\n",
    "DATASET_ROOT = \"../small_fire_detect\"\n",
    "\n",
    "# Dataset files\n",
    "train_images_dir = f\"{DATASET_ROOT}/images_train\"\n",
    "train_masks_dir = f\"{DATASET_ROOT}/masks_train\"\n",
    "test_images_dir = f\"{DATASET_ROOT}/images_val\"\n",
    "test_masks_dir = f\"{DATASET_ROOT}/masks_val\"\n",
    "\n",
    "def create_tf_dataset(images_dir, masks_dir):\n",
    "    def load_image(image_path, mask_path):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "        mask = tf.io.read_file(mask_path)\n",
    "        mask = tf.image.decode_png(mask, channels=1)\n",
    "        mask = tf.cast(mask, tf.uint8)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    image_paths = [os.path.join(images_dir, fname) for fname in sorted(os.listdir(images_dir))]\n",
    "    mask_paths = [os.path.join(masks_dir, fname) for fname in sorted(os.listdir(masks_dir))]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_ds(inputs, batch_size = BATCH_SIZE, augment=False):\n",
    "    def binary_mask(mask):\n",
    "        mask = mask / 255\n",
    "        return tf.cast(mask, dtype= tf.uint8)\n",
    "\n",
    "    def resize(image, mask):\n",
    "        image = tf.image.resize(image, [MODEL_HEIGHT, MODEL_WIDTH])\n",
    "        mask = tf.image.resize(mask, [MODEL_HEIGHT, MODEL_WIDTH], method='nearest')\n",
    "        return {\"images\": image, \"segmentation_masks\": binary_mask(mask)}\n",
    "\n",
    "    dataset = inputs.map(resize, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size = len(inputs))\n",
    "    if augment:\n",
    "        dataset = dataset.map(apply_augmentation_pipeline, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size, drop_remainder = True).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "# Create the datasets\n",
    "train_ds = create_tf_dataset(train_images_dir, train_masks_dir)\n",
    "eval_ds = create_tf_dataset(test_images_dir, test_masks_dir)\n",
    "\n",
    "# Pre-process/Augment\n",
    "train_ds = preprocess_ds(train_ds, augment= True)\n",
    "eval_ds = preprocess_ds(eval_ds, augment= False)\n",
    "\n",
    "\n",
    "# Prepare for training\n",
    "def dict_to_tuple(x):\n",
    "    return x[\"images\"], tf.one_hot(\n",
    "        tf.cast(tf.squeeze(x[\"segmentation_masks\"], axis=-1), \"int32\"), 2\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(dict_to_tuple)\n",
    "eval_ds = eval_ds.map(dict_to_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_cb = ModelCheckpoint(\n",
    "    filepath='best_unet.weights.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model = unet_builder((MODEL_HEIGHT, MODEL_WIDTH, 3),\n",
    "                   NUM_CLASSES,\n",
    "                   weights= None)\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_ds, validation_data=eval_ds, epochs=EPOCHS, callbacks = [saving_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, title=''):\n",
    "    metrics = ['loss', 'binary_iou', 'accuracy'] if NUM_CLASSES == 2 else ['loss', 'mean_iou', 'categorical_accuracy']\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(1, len(metrics), i + 1)\n",
    "        plt.plot(history.history[metric], label='train')\n",
    "        plt.plot(history.history['val_' + metric], label='val')\n",
    "        plt.title(metric)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(history, 'Segmentation head training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune backbone on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_full_model(model, train_ds, val_ds, epochs=10,num_classes = 2, learning_rate = 0.0001, callbacks=None):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    loss_fn, model_metric, model_metric2, _ = get_configs(num_classes)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=loss_fn,\n",
    "                  metrics=[model_metric, model_metric2])\n",
    "\n",
    "    history = model.fit(train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "    return history\n",
    "\n",
    "history = finetune_full_model(model, train_ds, eval_ds, epochs=10, learning_rate= 0.0001, callbacks=[saving_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "def plot_segmentation_results(model, dataset, num_images=4, num_classes=21, normalized=False):\n",
    "    def overlay_segmentation(image, mask):\n",
    "\n",
    "        image = np.squeeze(image, axis = 0)\n",
    "        # Create a copy of the image to overlay green on the mask regions\n",
    "        overlay = image.copy()\n",
    "        \n",
    "        # Apply green color (set green channel to 255) where the mask is non-zero\n",
    "        overlay[mask != 0] = [0, 255, 0]\n",
    "        \n",
    "        return overlay\n",
    "\n",
    "    images = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Get the specified number of images and their corresponding predictions\n",
    "    for img, _ in dataset.take(num_images):\n",
    "        if normalized:\n",
    "            image = img * 255\n",
    "        images.append(image.numpy().astype(np.uint8))\n",
    "        \n",
    "        # Run inference\n",
    "        pred_mask = model.predict(img)\n",
    "        pred_mask = np.squeeze(pred_mask, axis=0)\n",
    "        pred_classes = np.argmax(pred_mask, axis=-1)\n",
    "        predictions.append(pred_classes)\n",
    "    \n",
    "    # Plot the images and their segmentation results in a grid\n",
    "    plt.figure(figsize=(12, 6 * num_images // 2))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_images // 2, 4, 2 * i + 1)\n",
    "        plt.imshow(images[i][0])\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.subplot(num_images // 2, 4, 2 * i + 2)\n",
    "        overlay_img = overlay_segmentation(images[i], predictions[i])\n",
    "        plt.imshow(overlay_img)\n",
    "        plt.title(\"Segmented Image\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_segmentation_results(model, eval_ds, num_images=8, num_classes=2, normalized=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
