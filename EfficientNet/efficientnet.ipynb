{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check original model from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import EfficientNetB0\n",
    "\n",
    "model_eff = EfficientNetB0()\n",
    "model_eff.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom blocks for EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package=\"efficient_net\", name=\"ConvBlock\")\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    \"Simple convolutional block with Conv2D + Batch Normalization + Activation (SiLu)\"\n",
    "    def __init__(self, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "\n",
    "        self.conv = layers.Conv2D(out_channels, kernel_size, strides=stride, padding=padding, use_bias=False)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.activation = layers.Activation(tf.nn.silu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.conv.build(input_shape)\n",
    "        self.bn.build(self.conv.compute_output_shape(input_shape))\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        out_shape = self.conv.compute_output_shape(input_shape)\n",
    "        return out_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"out_channels\": self.out_channels,\n",
    "                \"kernel_size\": self.kernel_size,\n",
    "                \"stride\": self.stride,\n",
    "                \"stride\": self.stride,\n",
    "                \"padding\": self.padding,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        return cls(**config)\n",
    "\n",
    "@keras.saving.register_keras_serializable(package=\"efficient_net\", name=\"SqueezeExcite\")\n",
    "class SqueezeExcitation(tf.keras.layers.Layer):\n",
    "    \"Squeeze-and-Excitation block for per-channel weighting of the feature maps\"\n",
    "    def __init__(self, in_channels, reduced_dim):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_dim = reduced_dim\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
    "        self.conv1 = layers.Conv2D(reduced_dim, kernel_size=1, activation=tf.nn.silu, use_bias=True)\n",
    "        self.conv2 = layers.Conv2D(in_channels, kernel_size=1, activation='sigmoid', use_bias= True)\n",
    "        self.reshape = layers.Reshape((1, 1, in_channels))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.global_avg_pool(inputs)\n",
    "        x = self.reshape(x) # Reshape back to (batch_size,1,1, in_channels)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return layers.Multiply()([inputs, x])\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.conv1.build((None, 1, 1, self.in_channels))\n",
    "        self.conv2.build((None, 1, 1, self.reduced_dim))\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"in_channels\": self.in_channels,\n",
    "                \"reduced_dim\": self.reduced_dim,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        return cls(**config)\n",
    "    \n",
    "@keras.saving.register_keras_serializable(package=\"efficient_net\", name=\"MBConvBlock\")\n",
    "class MBBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"MBConv block with skip connections\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, expand_rate, reduction=4, dropout_rate=0.2):\n",
    "        super(MBBlock, self).__init__()\n",
    "\n",
    "        # Paramateres\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.expand_rate = expand_rate\n",
    "        self.reduction = reduction\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "\n",
    "        self.hidden_dim = self.in_channels * self.expand_rate\n",
    "        self.expand = self.in_channels != self.hidden_dim\n",
    "        reduced_dim = int(self.in_channels / self.reduction)\n",
    "\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = ConvBlock(self.hidden_dim, kernel_size=1, stride=1, padding=padding)\n",
    "\n",
    "        self.conv = tf.keras.Sequential([\n",
    "            layers.DepthwiseConv2D(kernel_size, stride, padding=padding, use_bias= False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation(tf.nn.silu),\n",
    "            SqueezeExcitation(self.hidden_dim, reduced_dim),\n",
    "            layers.Conv2D(out_channels, kernel_size=1, strides=1, padding=padding, use_bias= False),\n",
    "            layers.BatchNormalization()\n",
    "        ])\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.skip_add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if self.expand:\n",
    "            x = self.expand_conv(inputs)\n",
    "        else:\n",
    "            x = inputs\n",
    "\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.stride == 1 and inputs.shape[-1] == x.shape[-1]:\n",
    "            x = self.skip_add([x, inputs])\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self.expand:\n",
    "            self.expand_conv.build(input_shape)\n",
    "            self.conv.build(self.expand_conv.compute_output_shape(input_shape))\n",
    "        else:\n",
    "            self.conv.build((input_shape))\n",
    "\n",
    "        self.built = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"in_channels\": self.in_channels,\n",
    "                \"out_channels\": self.out_channels,\n",
    "                \"kernel_size\": self.kernel_size,\n",
    "                \"stride\": self.stride,\n",
    "                \"padding\": self.padding,\n",
    "                \"expand_rate\": self.expand_rate,\n",
    "                \"reduction\": self.reduction,\n",
    "                \"dropout_rate\": self.dropout_rate\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build EfficientNetB0 from scratch and compare to model from original paper (~5.3m parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@keras.saving.register_keras_serializable(package=\"efficient_net\", name=\"EfficientNet\")\n",
    "class EfficientNet(keras.Model):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape,\n",
    "            num_classes,\n",
    "            name = 'EfficientNet',\n",
    "            include_head = True,\n",
    "            dropout = 0.2,\n",
    "            width_factor = 1.0,\n",
    "            depth_factor = 1.0\n",
    "    ):\n",
    "        \"\"\"EfficientNet builder class\"\"\"\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        self.include_head = include_head\n",
    "        self.dropout = dropout\n",
    "        self.width_factor = width_factor\n",
    "        self.depth_factor = depth_factor\n",
    "        \n",
    "        # Model blocks\n",
    "        self.feature_extractor = self.build_feature_head(self.input_shape,\n",
    "                                                         self.width_factor,\n",
    "                                                         self.depth_factor)\n",
    "        \n",
    "        if include_head:\n",
    "            self.mlp = keras.Sequential(\n",
    "                [   layers.GlobalAveragePooling2D(name='feature_pooling'),\n",
    "                    layers.Dropout(self.dropout, name ='classifier_dropout'),\n",
    "                    layers.Dense(self.num_classes, activation='softmax', name='classifier')\n",
    "                ], name = 'classifier_head'\n",
    "            )\n",
    "\n",
    "\n",
    "    def call(self, inputs, training= False):\n",
    "        x = self.feature_extractor(inputs, training = training)\n",
    "\n",
    "        if self.include_head:\n",
    "            x = self.mlp(x, training = training)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build_functional(self):\n",
    "        input_layer = layers.Input(self.input_shape, name = 'Input')\n",
    "        output_layer = self(input_layer)\n",
    "        return keras.Model(inputs=input_layer, outputs=output_layer, name = self.name)\n",
    "    \n",
    "    def build_feature_head(self, input_shape, width_factor, depth_factor):\n",
    "\n",
    "        # Baseline EfficientNetB0 parameters\n",
    "        basic_mb_params = [\n",
    "            # expansion_rate, channels(c), repeats(t), stride(s), kernel_size(k)\n",
    "            [1, 16, 1, 1, 3],\n",
    "            [6, 24, 2, 2, 3],\n",
    "            [6, 40, 2, 2, 5],\n",
    "            [6, 80, 3, 2, 3],\n",
    "            [6, 112, 3, 1, 5],\n",
    "            [6, 192, 4, 2, 5],\n",
    "            [6, 320, 1, 1, 3],\n",
    "        ]\n",
    "\n",
    "        # Calculate the filte size of the last convolutional block\n",
    "        last_conv_dims = ceil(1280 * width_factor)\n",
    "\n",
    "        inputs = layers.Input(shape=input_shape, name = 'Input')\n",
    "        channels = int(32 * width_factor)\n",
    "        x = ConvBlock(channels, 3, stride=2, padding='same')(inputs)\n",
    "        in_channels = channels\n",
    "\n",
    "        for r, c_o, repeat, s, k in basic_mb_params:\n",
    "            out_channels = 4 * ceil(int(c_o * width_factor) / 4)\n",
    "            num_layers = ceil(repeat * depth_factor)\n",
    "\n",
    "            for layer in range(num_layers):\n",
    "                # On layers with more than 1 repetitions, only the first get Depthwise with stride >1\n",
    "                stride = s if layer == 0 else 1\n",
    "                x = MBBlock(in_channels, out_channels, kernel_size=k, stride=stride, padding='same', expand_rate=r)(x)\n",
    "                in_channels = out_channels\n",
    "                \n",
    "        x = ConvBlock(last_conv_dims, kernel_size=1, stride=1, padding='valid')(x)\n",
    "        feature_extractor = keras.Model(inputs, x, name='feature_extractor')\n",
    "        return feature_extractor\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"input_shape\": self.input_shape,\n",
    "                \"num_classes\": self.num_classes,\n",
    "                \"name\": self.name,\n",
    "                \"include_head\": self.include_head,\n",
    "                \"dropout\": self.dropout,\n",
    "                \"width_factor\": self.width_factor,\n",
    "                \"depth_factor\": self.depth_factor\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config.pop('trainable')\n",
    "        config.pop('dtype')\n",
    "        return cls(**config)\n",
    "    \n",
    "model = EfficientNet(input_shape=(224,224,3),\n",
    "                     num_classes= 1000,\n",
    "                     name = 'EfficientNetB0',\n",
    "                     include_head= True\n",
    "                    )\n",
    "model(np.expand_dims(np.random.rand(224,224,3),axis = 0))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def preprocess_image(image, label, image_size):\n",
    "    image = tf.image.resize(image, (image_size, image_size))\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "def load_data(image_size, batch_size):\n",
    "    (train_ds, val_ds), ds_info = tfds.load(\n",
    "        'cifar10',\n",
    "        split=['train', 'test'],\n",
    "        as_supervised=True,\n",
    "        with_info=True\n",
    "    )\n",
    "\n",
    "    num_classes = ds_info.features['label'].num_classes\n",
    "\n",
    "    train_ds = train_ds.map(lambda image, label: preprocess_image(image, label, image_size))\n",
    "    val_ds = val_ds.map(lambda image, label: preprocess_image(image, label, image_size))\n",
    "\n",
    "    train_ds = train_ds.shuffle(buffer_size=1000).batch(batch_size, drop_remainder = True).prefetch(AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size, drop_remainder = True).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, ds_info, num_classes\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 128\n",
    "ds_train, ds_test, ds_info, num_classes = load_data(image_size, batch_size)\n",
    "\n",
    "\n",
    "model = EfficientNet(input_shape =(image_size,image_size,3),\n",
    "                     num_classes = num_classes,\n",
    "                     name = 'EfficientNetB0',\n",
    "                     include_head = True\n",
    "                    ).build_functional()\n",
    "model.summary()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# Prepare training arguments\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "initial_learning_rate = 1e-3\n",
    "decay_steps = len(ds_train) * (50 - 10)\n",
    "alpha = 1e-5 / initial_learning_rate\n",
    "warmup_steps = len(ds_train) * 10\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps, warmup_target=1e-5,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=5e-5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "epochs = 50\n",
    "history = model.fit(ds_train, epochs=epochs, validation_data=ds_test, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results and training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predictions(model, dataset, ds_info, num_images=8):\n",
    "    class_names = {idx: name for idx, name in enumerate(ds_info.features['label'].names)}\n",
    "\n",
    "    # Get a random sample of num_images from the dataset\n",
    "    random_indices = np.random.choice(len(dataset), size=num_images, replace=False)\n",
    "    ds_subset = dataset.unbatch().skip(random_indices[0]).take(num_images).batch(num_images)\n",
    "\n",
    "    # Make predictions\n",
    "    images, labels = next(iter(ds_subset))\n",
    "    predicted_logits = model.predict(images)\n",
    "    predicted_probabilities = tf.nn.softmax(predicted_logits, axis=-1)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=-1)\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow((images[i].numpy() * 255).astype(np.uint8))\n",
    "        if predicted_classes[i] == labels[i].numpy():\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "\n",
    "        plt.title(f'True: {class_names[labels[i].numpy()]}\\nPredicted: {class_names[predicted_classes[i]]}', color=color)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "random_predictions(model, ds_train, ds_info, num_images=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
