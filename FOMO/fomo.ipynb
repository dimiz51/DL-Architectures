{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the FOMO model with pre-trained backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def build_fomo_model(input_shape=(224, 224, 3), num_classes=2):\n",
    "    \"\"\"\n",
    "    Builds a FOMO (Faster Objects, More Objects) model using a truncated MobileNetV2 as the feature extractor \n",
    "    and a grid-based classifier head for object detection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple, optional\n",
    "        Shape of the input image, default is (224, 224, 3), where 224x224 are the height and width in pixels, \n",
    "        and 3 corresponds to the number of channels (RGB).\n",
    "    \n",
    "    num_classes : int, optional\n",
    "        Number of object classes for classification and localization (can be binary or multiclass but we always\n",
    "        need a background class)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : tf.keras.Model\n",
    "        A TensorFlow Keras model object that can be used for object detection tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the MobileNetV2 model pre-trained on ImageNet\n",
    "    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "    # To keep the model simple we are cutting the network to an early feature extraction layer (block_6_expand_relu)\n",
    "    # block_6_expand_relu output shape is (None, 20, 20, 96)\n",
    "    feature_extractor = models.Model(inputs=base_model.input, \n",
    "                                     outputs=base_model.get_layer('block_6_expand_relu').output,\n",
    "                                     name = 'MobileNetV2_Block6')\n",
    "    \n",
    "    # Freeze the layers of the backbone\n",
    "    feature_extractor.trainable = False\n",
    "\n",
    "    # Input layer\n",
    "    inputs = tf.keras.Input(shape=input_shape, name = 'FOMO_input')\n",
    "    \n",
    "    # Pass input through the feature extractor\n",
    "    x = feature_extractor(inputs)\n",
    "    \n",
    "    #-----------FOMO head grid based classifier----------\n",
    "    x = layers.Conv2D(16, (3, 3), padding='same', activation='relu', name='conv_1')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), padding='same', activation='relu', name='conv_2')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = layers.Conv2D(num_classes, (3, 3), padding='same', activation='softmax', name='output')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Model(inputs=inputs, outputs=logits, name = 'FOMO')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "model = build_fomo_model(num_classes=2)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads the chess pieces dataset from Roboflow and create a Tensorflow dataset generator\n",
    "\n",
    "Link: https://public.roboflow.com/object-detection/chess-full/\n",
    "\n",
    "**Important Note**: Make sure you select the Pytorch YoloV5 version of the dataset when exporting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"CURL LINK\" -o roboflow.zip\n",
    "!unzip roboflow.zip -d chess_ds\n",
    "!rm roboflow.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensorflow Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = 224\n",
    "\n",
    "def load_image_and_bboxes(image_path, label_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (MODEL_SIZE, MODEL_SIZE))\n",
    "\n",
    "    # Load bounding boxes\n",
    "    bboxes = []\n",
    "    classes = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            parts = list(map(float, line.strip().split()))\n",
    "\n",
    "            # We just treat every chess piece as one class for simplicity, but feel\n",
    "            # free to use the original classes.\n",
    "            # Set class index to 1 for single class detection, 0 as background\n",
    "            class_idx = 1\n",
    "            x_center, y_center, width, height = parts[1:5]\n",
    "            bboxes.append([x_center, y_center, width, height])\n",
    "            classes.append([class_idx])\n",
    "\n",
    "    return image, bboxes, classes\n",
    "\n",
    "def create_tf_dataset(image_dir, label_dir):\n",
    "    image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith('.jpg')]\n",
    "    label_paths = [os.path.join(label_dir, filename[:-4] + '.txt')for filename in os.listdir(image_dir) if filename.endswith('.jpg')]\n",
    "\n",
    "    def generator():\n",
    "        for img_path, lbl_path in zip(image_paths, label_paths):\n",
    "            image, bboxes, classes = load_image_and_bboxes(img_path, lbl_path)\n",
    "            if len(bboxes) == 0:\n",
    "                continue  # Skip if no bounding boxes\n",
    "            image = tf.convert_to_tensor(image, dtype=tf.float32) / 255.0\n",
    "            bboxes = tf.ragged.constant(bboxes, dtype=tf.float32)\n",
    "            classes = tf.ragged.constant(classes, dtype=tf.int64)\n",
    "            yield {'image': image, 'bboxes': bboxes, 'classes': classes}\n",
    "\n",
    "    # Create TensorFlow Dataset from generator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature={\n",
    "            'image': tf.TensorSpec(shape=(MODEL_SIZE, MODEL_SIZE, 3), dtype=tf.float32),\n",
    "            'bboxes': tf.RaggedTensorSpec(shape=(None, None), dtype=tf.float32),\n",
    "            'classes': tf.RaggedTensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Load training and validation datasets\n",
    "train_dataset = create_tf_dataset(\"chess_ds/train/images\", \"chess_ds/train/labels\")\n",
    "val_dataset = create_tf_dataset(\"chess_ds/valid/images\", \"chess_ds/valid/labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bboxes(image, bboxes, labels):\n",
    "    \"\"\"\n",
    "    Visualizes bounding boxes on the image.\n",
    "\n",
    "    Args:\n",
    "        image (tf.Tensor): The image tensor (height, width, channels).\n",
    "        bboxes (tf.Tensor): The bounding boxes tensor (num_boxes, 4), where each box is [cx, cy, w, h].\n",
    "        labels (tf.Tensor): The labels tensor (num_boxes, 1) or (num_boxes,).\n",
    "    \"\"\"\n",
    "    # Convert the image tensor to a numpy array and scale if necessary\n",
    "    image = image.numpy()\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Get the number of boxes\n",
    "    num_boxes = tf.shape(bboxes)[0]\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        # Get the center coordinates and dimensions\n",
    "        cx, cy, w, h = bboxes[i].numpy() * MODEL_SIZE\n",
    "\n",
    "        # Calculate the top-left corner of the box\n",
    "        x_min = int(cx - w / 2)\n",
    "        y_min = int(cy - h / 2)\n",
    "\n",
    "        # Draw the rectangle\n",
    "        rect = plt.Rectangle((x_min, y_min), w, h, fill=False, color='blue', linewidth=4)\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "        # Add label text\n",
    "        plt.text(x_min, y_min,\"chess_piece\", color='white', fontsize=12,\n",
    "                 bbox=dict(facecolor='blue', alpha=0.5))\n",
    "\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    plt.show()\n",
    "\n",
    "# Example usage: Visualizing a single image and its bounding boxes from the training dataset\n",
    "for sample in train_dataset.take(1):\n",
    "    visualize_bboxes(sample['image'], sample['bboxes'], sample['classes'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack dataset from dict version and convert RaggedTensors to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unpack image, bounding boxes, and labels from the dataset\n",
    "def unpack_info(sample):\n",
    "    image = sample['image']\n",
    "    labels = sample['classes'].to_tensor()\n",
    "    bboxes = sample['bboxes'].to_tensor()\n",
    "    return image, bboxes, labels\n",
    "\n",
    "# Prepare the dataset and unpack the data\n",
    "train_dataset = train_dataset.map(unpack_info, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(unpack_info, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to grid-styled label annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_grid_labels(image, bboxes, labels, grid_size=28):\n",
    "    \"\"\"Convert bounding boxes to grid-style centroids label map\"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    cell_height = height / grid_size\n",
    "    cell_width = width / grid_size\n",
    "\n",
    "    # Create a temporary grid of zeros for labels\n",
    "    temp_grid_labels = tf.zeros((grid_size, grid_size), dtype=tf.int32)\n",
    "\n",
    "    # Use the center coordinates directly from bboxes\n",
    "    center_x = bboxes[:, 0] * MODEL_SIZE  # bboxes[:, 0] is center_x\n",
    "    center_y = bboxes[:, 1] * MODEL_SIZE  # bboxes[:, 1] is center_y\n",
    "\n",
    "    # Calculate grid cell indices\n",
    "    grid_y = tf.floor(center_y / cell_height)\n",
    "    grid_x = tf.floor(center_x / cell_width)\n",
    "\n",
    "    # Clip values to be within the grid size\n",
    "    grid_y = tf.clip_by_value(grid_y, 0, grid_size - 1)\n",
    "    grid_x = tf.clip_by_value(grid_x, 0, grid_size - 1)\n",
    "\n",
    "    # Prepare the indices to update the grid\n",
    "    indices = tf.cast(tf.stack([grid_y, grid_x], axis=-1), dtype=tf.int32)\n",
    "\n",
    "    # Use tf.tensor_scatter_nd_update to assign labels directly\n",
    "    updates = tf.squeeze(labels, axis=-1)\n",
    "\n",
    "    # Scatter updates to the grid labels\n",
    "    temp_grid_labels = tf.tensor_scatter_nd_update(temp_grid_labels, indices, tf.cast(updates, dtype = tf.int32))\n",
    "\n",
    "    return image, temp_grid_labels\n",
    "\n",
    "# Convert boxes to the 28x28 grid style centroid labels representation \n",
    "labeled_train = train_dataset.map(lambda image, bboxes, labels: assign_grid_labels(image, bboxes, labels), \n",
    "                                   num_parallel_calls=tf.data.AUTOTUNE)\n",
    "labeled_val = val_dataset.map(lambda image, bboxes, labels: assign_grid_labels(image, bboxes, labels), \n",
    "                               num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Preview the first few labeled examples\n",
    "# for img, grid in labeled_train.take(1):\n",
    "#     print(\"Image shape:\", img.shape)\n",
    "#     print(\"Grid labels:\", grid.numpy())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify grid-styled labels after conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_non_zero_centers(image, grid, grid_size=28):\n",
    "    \"\"\"Plot the grid centers where class index is not background\"\"\"\n",
    "    height, width, _ = image.shape\n",
    "    cell_height = height / grid_size\n",
    "    cell_width = width / grid_size \n",
    "\n",
    "    # Create a copy of the image to plot on\n",
    "    image_with_centers = image.copy()\n",
    "\n",
    "    # Iterate through the grid to find non-zero class indices\n",
    "    for y in range(grid_size):\n",
    "        for x in range(grid_size):\n",
    "            if grid[y, x] != 0:  # Check if the class index is not zero\n",
    "                # Calculate the center of the cell to plot\n",
    "                center_x = int((x + 0.5) * cell_width)\n",
    "                center_y = int((y + 0.5) * cell_height)\n",
    "\n",
    "                # Draw a green circle at the center location\n",
    "                cv2.circle(image_with_centers, (center_x, center_y), radius=5, color=(0, 255, 0), thickness=-1)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_with_centers.astype(np.uint8))\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.title('Ground truth object centers')\n",
    "    plt.show()\n",
    "\n",
    "num_images_to_display = 5\n",
    "\n",
    "# Create a loop to display multiple images\n",
    "for i, (img, grid) in enumerate(labeled_train.take(num_images_to_display)):\n",
    "    # Ensure the image is in the correct format and scale it properly\n",
    "    image_for_plot = img.numpy() * 255.0\n",
    "    \n",
    "    # Plot the non-zero centers for the current image\n",
    "    plot_non_zero_centers(image_for_plot.astype(np.uint8), grid.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = labeled_train.batch(batch_size, drop_remainder= True).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = labeled_val.batch(batch_size, drop_remainder= True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert grid-styled detections to centroid and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids_from_heatmap(heatmap, threshold=0.5, rescale=8.0):\n",
    "    \"\"\"Convert output heatmap to non-background object centroids\"\"\"\n",
    "    print(f'Rescaling by {rescale}...')\n",
    "\n",
    "    heatmap = heatmap.squeeze()\n",
    "\n",
    "    # Get class probabilities for all classes\n",
    "    class_probabilities = heatmap[..., 0:] \n",
    "\n",
    "    centroids = []\n",
    "    grid_size = heatmap.shape[0]\n",
    "    cell_size = rescale\n",
    "\n",
    "    for i in range(grid_size): # Y\n",
    "        for j in range(grid_size): # X\n",
    "            # Get the maximum probability and the index of that probability\n",
    "            max_prob = np.max(class_probabilities[i, j,:]) \n",
    "            max_prob_index = np.argmax(class_probabilities[i, j, :])\n",
    "\n",
    "            # Exclude background\n",
    "            if max_prob_index == 0:\n",
    "                continue\n",
    "\n",
    "            if max_prob >= threshold:\n",
    "                cx = int((j + 0.5) * cell_size)\n",
    "                cy = int((i + 0.5) * cell_size) \n",
    "                centroids.append((cx, cy))\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def plot_detections(image, centroids):\n",
    "    \"\"\"Plot the predicted object centers\"\"\"\n",
    "    image = image * 255\n",
    "\n",
    "    image = np.clip(image, 0, 255)\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Plot each centroid\n",
    "    for cx, cy in centroids:\n",
    "        plt.plot(cx, cy, 'ro', markersize=10)\n",
    "\n",
    "    plt.imshow(image.astype(np.uint8))\n",
    "\n",
    "    plt.title('Predicted object centers')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in val_dataset.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    \n",
    "    # Get the batch size\n",
    "    batch_size = predictions.shape[0]  \n",
    "\n",
    "    # Iterate over each image and its corresponding prediction in the batch\n",
    "    for i in range(batch_size):\n",
    "        print(f\"Processing image {i + 1}/{batch_size}\")\n",
    "        img = images[i]\n",
    "        pred = predictions[i]\n",
    "        \n",
    "        # Calculate the rescaling factor based on the prediction grid size and the original image size (assumed 224x224 here)\n",
    "        rescale_factor = images.shape[1] / pred.shape[0]\n",
    "        \n",
    "        # Get the predicted (y,x) centroids\n",
    "        centroids = get_centroids_from_heatmap(pred, threshold=0.5, rescale=rescale_factor)\n",
    "        print(f\"Detected centroids for image {i + 1}: {centroids}\")\n",
    "        print(f\"Number of detected centroids for image {i + 1}: {len(centroids)}\")\n",
    "        \n",
    "        # Plot the centroids on the image\n",
    "        plot_detections(img, centroids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
